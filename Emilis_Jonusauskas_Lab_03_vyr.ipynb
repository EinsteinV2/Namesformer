{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tygdtwy7yxAf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import unicodedata\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = []\n",
        "for key in ['a', 'b', 'c', 'c-2', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "            'm', 'n', 'o', 'p', 'r', 's', 's-2', 't', 'u', 'v', 'z', 'z-2']:\n",
        "    url = f'https://vardai.vlkk.lt/sarasas/{key}/'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    links = soup.find_all('a', class_='names_list__links names_list__links--man')\n",
        "    names += [name.text for name in links]\n",
        "\n",
        "np.savetxt('vardai.txt', names, fmt='%s', header='name', comments='', newline='\\n')"
      ],
      "metadata": {
        "id": "H7pBgSU_y-LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Panaikinau kirčius, kad nesigautų vardų kaip Siontr̃rovyìs.\n",
        "Taip pat panaikinau didžiąsias raides , dėl kodo efektyvumo."
      ],
      "metadata": {
        "id": "CU3sWoMWNhOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NameDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        # Load and preprocess names\n",
        "        self.names = self._preprocess_names(pd.read_csv(csv_file)['name'].values)\n",
        "\n",
        "        # Build vocabulary (characters + padding space)\n",
        "        lithuanian_letters = \"ąčęėįšųū\"\n",
        "        self.chars = sorted(list(set(''.join(self.names)+ lithuanian_letters + ' ')))  # Including a padding character\n",
        "        self.char_to_int = {c: i for i, c in enumerate(self.chars)}\n",
        "        self.int_to_char = {i: c for c, i in self.char_to_int.items()}\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "    def _preprocess_names(self, names):\n",
        "        \"\"\"Removes accentuation and normalizes the names.\"\"\"\n",
        "        return [\n",
        "            ''.join(\n",
        "                c for c in unicodedata.normalize('NFD', name)\n",
        "                if unicodedata.category(c) != 'Mn'\n",
        "            ).lower()\n",
        "            for name in names\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Add a padding character at the end\n",
        "        name = self.names[idx] + ' '\n",
        "        # Encode the name into integers\n",
        "        encoded_name = [self.char_to_int[char] for char in name]\n",
        "        return torch.tensor(encoded_name)"
      ],
      "metadata": {
        "id": "uq3nryhezBdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = NameDataset('vardai.txt')\n",
        "\n",
        "# Custom collate function for padding\n",
        "def pad_collate(batch):\n",
        "    padded_seqs = pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "    input_seq = padded_seqs[:, :-1]\n",
        "    target_seq = padded_seqs[:, 1:]\n",
        "    return input_seq, target_seq\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=pad_collate)"
      ],
      "metadata": {
        "id": "ebLDfCAV9aUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epochų skaičių palikau ties 50, nes su didesniu skaičiu vardai tampa prasti"
      ],
      "metadata": {
        "id": "7U3vi0hXsKIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MinimalTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, forward_expansion):\n",
        "        super(MinimalTransformer, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_size))\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n",
        "        self.output_layer = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.size(1)).unsqueeze(0)\n",
        "        x = self.embed(x) + self.positional_encoding[:, :x.size(1), :]\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# Training Loop\n",
        "def train_model(model, dataloader, epochs=50):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Ensure the model is in training mode\n",
        "        total_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_seq)\n",
        "            loss = criterion(output.transpose(1, 2), target_seq)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        average_loss = total_loss / batch_count\n",
        "        print(f'Epoch {epoch+1}, Average Loss: {average_loss}')\n",
        "\n",
        "model = MinimalTransformer(vocab_size=dataset.vocab_size, embed_size=128, num_heads=8, forward_expansion=4)\n",
        "train_model(model, dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nLshd1rzplv",
        "outputId": "ac22e394-f811-42a1-9974-8188aa3f7dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 1.3011297884066242\n",
            "Epoch 2, Average Loss: 1.16333576224067\n",
            "Epoch 3, Average Loss: 1.1374777299313505\n",
            "Epoch 4, Average Loss: 1.139126143179649\n",
            "Epoch 5, Average Loss: 1.1274003553981624\n",
            "Epoch 6, Average Loss: 1.1301050196009235\n",
            "Epoch 7, Average Loss: 1.119457402012565\n",
            "Epoch 8, Average Loss: 1.120065768888174\n",
            "Epoch 9, Average Loss: 1.1212796278236326\n",
            "Epoch 10, Average Loss: 1.117823187969933\n",
            "Epoch 11, Average Loss: 1.1093977118326612\n",
            "Epoch 12, Average Loss: 1.1058679247690626\n",
            "Epoch 13, Average Loss: 1.1042931439462773\n",
            "Epoch 14, Average Loss: 1.10312314959597\n",
            "Epoch 15, Average Loss: 1.101318230313703\n",
            "Epoch 16, Average Loss: 1.1069104129617864\n",
            "Epoch 17, Average Loss: 1.1092007303040874\n",
            "Epoch 18, Average Loss: 1.1016023188583122\n",
            "Epoch 19, Average Loss: 1.1042215533492978\n",
            "Epoch 20, Average Loss: 1.100310335474566\n",
            "Epoch 21, Average Loss: 1.1083885997780099\n",
            "Epoch 22, Average Loss: 1.0956577008420771\n",
            "Epoch 23, Average Loss: 1.0988395534271052\n",
            "Epoch 24, Average Loss: 1.0996263027191162\n",
            "Epoch 25, Average Loss: 1.0890200758768507\n",
            "Epoch 26, Average Loss: 1.1043010363894061\n",
            "Epoch 27, Average Loss: 1.0972316279884213\n",
            "Epoch 28, Average Loss: 1.1013725005890713\n",
            "Epoch 29, Average Loss: 1.0986626650676254\n",
            "Epoch 30, Average Loss: 1.0907727721308873\n",
            "Epoch 31, Average Loss: 1.0953627716411243\n",
            "Epoch 32, Average Loss: 1.103418803412067\n",
            "Epoch 33, Average Loss: 1.0878471333133288\n",
            "Epoch 34, Average Loss: 1.0955275361202965\n",
            "Epoch 35, Average Loss: 1.090577139834727\n",
            "Epoch 36, Average Loss: 1.095375991064655\n",
            "Epoch 37, Average Loss: 1.091294263512635\n",
            "Epoch 38, Average Loss: 1.0979260206222534\n",
            "Epoch 39, Average Loss: 1.0890867202735144\n",
            "Epoch 40, Average Loss: 1.0888545261926887\n",
            "Epoch 41, Average Loss: 1.093376391682743\n",
            "Epoch 42, Average Loss: 1.0868610843154025\n",
            "Epoch 43, Average Loss: 1.0958496913437015\n",
            "Epoch 44, Average Loss: 1.0902197124544255\n",
            "Epoch 45, Average Loss: 1.0863350787438637\n",
            "Epoch 46, Average Loss: 1.0932114917384692\n",
            "Epoch 47, Average Loss: 1.09450671594005\n",
            "Epoch 48, Average Loss: 1.096406307102235\n",
            "Epoch 49, Average Loss: 1.0868717205425924\n",
            "Epoch 50, Average Loss: 1.0951592365572276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Įdiegiau keletą papildomų sąlygų, kad vyriški vardai būtų lietuviški :\n",
        "\n",
        "\n",
        "1.   Ilgiausias vardas Lietuvoje yra Konstantinas - 12 raidžių , todėl modelis generuoja tik maksimaliai 13 raidžių vardus\n",
        "2.   Vardai turi baigtis raide 's'\n",
        "3.   Varduose prieš raidę 's' turi būti balsė.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iftlNiUSLnjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, dataset, start_str='a', max_length=13,temperature =1):\n",
        "    assert temperature > 0\n",
        "    model.eval()  # Switch to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Convert start string to tensor\n",
        "        chars = [dataset.char_to_int[c] for c in start_str]\n",
        "        input_seq = torch.tensor(chars).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        output_name = start_str\n",
        "        vowels = set(\"aeiouy\")  # Allowed vowels\n",
        "\n",
        "        for _ in range(max_length - len(start_str)):\n",
        "            output = model(input_seq)\n",
        "\n",
        "            # Get the last character from the output\n",
        "            logits = output[0, -1] / temperature\n",
        "            probabilities = torch.softmax(logits, dim=0)\n",
        "            # Sample a character from the probability distribution\n",
        "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
        "            next_char = dataset.int_to_char[next_char_idx]\n",
        "\n",
        "            if next_char == ' ':  # Assume ' ' is your end-of-sequence character\n",
        "                break\n",
        "\n",
        "            output_name += next_char\n",
        "            # Update the input sequence for the next iteration\n",
        "            input_seq = torch.cat([input_seq, torch.tensor([[next_char_idx]])], dim=1)\n",
        "\n",
        "        # Enforce the name ends with a vowel followed by 's'\n",
        "        if len(output_name) < max_length:\n",
        "            # If it doesn't already end with a vowel and 's', adjust it\n",
        "            if len(output_name) < 2 or output_name[-1] != 's' or output_name[-2] not in vowels:\n",
        "                last_vowel = random.choice(list(vowels))  # Randomly pick a vowel\n",
        "                output_name = output_name.rstrip()[:-1] + last_vowel + 's'\n",
        "\n",
        "        return output_name\n",
        "\n",
        "# After training your model, generate a name starting with a specific letter\n",
        "for _ in range(15):\n",
        "    generated_name = sample(model, dataset, start_str='e',temperature = 0.5)\n",
        "    print(generated_name.capitalize())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIG7SsLj410m",
        "outputId": "a802e448-54e5-4771-8260-4e870968d1be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emidlis\n",
            "Emidinas\n",
            "Emiris\n",
            "Emifelis\n",
            "Emisys\n",
            "Emidonas\n",
            "Emidis\n",
            "Emijas\n",
            "Eminantonas\n",
            "Emijutas\n",
            "Emimanijus\n",
            "Emiurtijus\n",
            "Emivys\n",
            "Emimas\n",
            "Emivydas\n"
          ]
        }
      ]
    }
  ]
}
